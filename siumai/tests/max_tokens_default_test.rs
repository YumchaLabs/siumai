#![cfg(all(
    feature = "openai",
    feature = "anthropic",
    feature = "google",
    feature = "ollama"
))]
//! Max Tokens Default Value Tests
//!
//! Tests to ensure all providers handle max_tokens defaults correctly.

use siumai::execution::transformers::request::RequestTransformer;
use siumai::types::{ChatRequest, CommonParams};

#[test]
fn test_anthropic_max_tokens_default() {
    let transformer =
        siumai::providers::anthropic::transformers::AnthropicRequestTransformer::default();

    // Test without max_tokens
    let params_without_max_tokens = CommonParams {
        model: "claude-3-5-sonnet-20241022".to_string(),
        temperature: Some(0.7),
        max_tokens: None, // No max_tokens provided
        max_completion_tokens: None,
        top_p: Some(0.9),
        stop_sequences: None,
        seed: None,
    };

    let req = ChatRequest::builder()
        .messages(Vec::new())
        .common_params(params_without_max_tokens)
        .build();
    let mapped = transformer.transform_chat(&req).unwrap();

    // Anthropic should automatically set default max_tokens
    assert_eq!(mapped["max_tokens"], 4096);

    // Test with explicit max_tokens
    let params_with_max_tokens = CommonParams {
        model: "claude-3-5-sonnet-20241022".to_string(),
        temperature: Some(0.7),
        max_tokens: Some(2000), // Explicit max_tokens
        max_completion_tokens: None,
        top_p: Some(0.9),
        stop_sequences: None,
        seed: None,
    };

    let req2 = ChatRequest::builder()
        .messages(Vec::new())
        .common_params(params_with_max_tokens)
        .build();
    let mapped_explicit = transformer.transform_chat(&req2).unwrap();

    // Should use the explicit value
    assert_eq!(mapped_explicit["max_tokens"], 2000);
}

#[test]
fn test_openai_max_tokens_optional() {
    let transformer = siumai::providers::openai::transformers::OpenAiRequestTransformer;

    // Test without max_tokens
    let params_without_max_tokens = CommonParams {
        model: "gpt-4".to_string(),
        temperature: Some(0.7),
        max_tokens: None, // No max_tokens provided
        max_completion_tokens: None,
        top_p: Some(0.9),
        stop_sequences: None,
        seed: None,
    };

    let req = ChatRequest::builder()
        .messages(Vec::new())
        .common_params(params_without_max_tokens)
        .build();
    let mapped = transformer.transform_chat(&req).unwrap();

    // OpenAI should not have max_tokens if not provided
    assert!(mapped.get("max_tokens").is_none());

    // Test with explicit max_tokens
    let params_with_max_tokens = CommonParams {
        model: "gpt-4".to_string(),
        temperature: Some(0.7),
        max_tokens: Some(2000), // Explicit max_tokens
        max_completion_tokens: None,
        top_p: Some(0.9),
        stop_sequences: None,
        seed: None,
    };

    let req2 = ChatRequest::builder()
        .messages(Vec::new())
        .common_params(params_with_max_tokens)
        .build();
    let mapped_explicit = transformer.transform_chat(&req2).unwrap();

    // Should use the explicit value
    assert_eq!(mapped_explicit["max_tokens"], 2000);
}

#[test]
fn test_gemini_max_tokens_optional() {
    let transformer = siumai::providers::gemini::transformers::GeminiRequestTransformer {
        config: siumai::providers::gemini::types::GeminiConfig::new("test")
            .with_model("gemini-1.5-pro".to_string())
            .with_http_config(siumai::types::HttpConfig::default()),
    };

    // Test without max_tokens
    let params_without_max_tokens = CommonParams {
        model: "gemini-1.5-pro".to_string(),
        temperature: Some(0.7),
        max_tokens: None, // No max_tokens provided
        max_completion_tokens: None,
        top_p: Some(0.9),
        stop_sequences: None,
        seed: None,
    };

    let req = ChatRequest::builder()
        .messages(Vec::new())
        .common_params(params_without_max_tokens)
        .build();
    let mapped = transformer.transform_chat(&req).unwrap();

    // Gemini should not have generationConfig.maxOutputTokens if not provided
    assert!(
        mapped
            .get("generationConfig")
            .and_then(|gc| gc.get("maxOutputTokens"))
            .is_none()
    );

    // Test with explicit max_tokens
    let params_with_max_tokens = CommonParams {
        model: "gemini-1.5-pro".to_string(),
        temperature: Some(0.7),
        max_tokens: Some(2000), // Explicit max_tokens
        max_completion_tokens: None,
        top_p: Some(0.9),
        stop_sequences: None,
        seed: None,
    };

    let req2 = ChatRequest::builder()
        .messages(Vec::new())
        .common_params(params_with_max_tokens)
        .build();
    let mapped_explicit = transformer.transform_chat(&req2).unwrap();

    // Should use the explicit value as generationConfig.maxOutputTokens
    assert_eq!(mapped_explicit["generationConfig"]["maxOutputTokens"], 2000);
}

#[test]
fn test_ollama_max_tokens_optional() {
    let transformer = siumai::providers::ollama::transformers::OllamaRequestTransformer {
        params: siumai::providers::ollama::config::OllamaParams::default(),
    };

    // Test without max_tokens
    let params_without_max_tokens = CommonParams {
        model: "llama3.2".to_string(),
        temperature: Some(0.7),
        max_tokens: None, // No max_tokens provided
        max_completion_tokens: None,
        top_p: Some(0.9),
        stop_sequences: None,
        seed: None,
    };

    let req = ChatRequest::builder()
        .messages(Vec::new())
        .common_params(params_without_max_tokens)
        .build();
    let mapped = transformer.transform_chat(&req).unwrap();

    // Ollama should not have num_predict if not provided (under options)
    assert!(
        mapped
            .get("options")
            .and_then(|o| o.get("num_predict"))
            .is_none()
    );

    // Test with explicit max_tokens
    let params_with_max_tokens = CommonParams {
        model: "llama3.2".to_string(),
        temperature: Some(0.7),
        max_tokens: Some(2000), // Explicit max_tokens
        max_completion_tokens: None,
        top_p: Some(0.9),
        stop_sequences: None,
        seed: None,
    };

    let req2 = ChatRequest::builder()
        .messages(Vec::new())
        .common_params(params_with_max_tokens)
        .build();
    let mapped_explicit = transformer.transform_chat(&req2).unwrap();

    // Should use the explicit value as options.num_predict
    assert_eq!(mapped_explicit["options"]["num_predict"], 2000);
}

#[test]
fn test_groq_max_tokens_optional() {
    use siumai::core::ProviderSpec;

    let spec = siumai::providers::groq::spec::GroqSpec;

    // Test without max_tokens (Groq uses OpenAI format)
    let params_without_max_tokens = CommonParams {
        model: "llama-3.3-70b-versatile".to_string(),
        temperature: Some(0.7),
        max_tokens: None, // No max_tokens provided
        max_completion_tokens: None,
        top_p: Some(0.9),
        stop_sequences: None,
        seed: None,
    };

    let req = ChatRequest::builder()
        .messages(Vec::new())
        .common_params(params_without_max_tokens)
        .build();
    let ctx = siumai::core::ProviderContext::new(
        "groq",
        "https://api.groq.com/openai/v1",
        None,
        Default::default(),
    );
    let bundle = spec.choose_chat_transformers(&req, &ctx);
    let mapped = bundle.request.transform_chat(&req).unwrap();

    // Groq (OpenAI format) should not have max_tokens if not provided
    assert!(mapped.get("max_tokens").is_none());

    // Test with explicit max_tokens
    let params_with_max_tokens = CommonParams {
        model: "llama-3.3-70b-versatile".to_string(),
        temperature: Some(0.7),
        max_tokens: Some(2000), // Explicit max_tokens
        max_completion_tokens: None,
        top_p: Some(0.9),
        stop_sequences: None,
        seed: None,
    };

    let req2 = ChatRequest::builder()
        .messages(Vec::new())
        .common_params(params_with_max_tokens)
        .build();
    let bundle2 = spec.choose_chat_transformers(&req2, &ctx);
    let mapped_explicit = bundle2.request.transform_chat(&req2).unwrap();

    // Should use the explicit value
    assert_eq!(mapped_explicit["max_tokens"], 2000);
}

#[test]
fn test_xai_max_tokens_optional() {
    use siumai::core::ProviderSpec;

    let spec = siumai::providers::xai::spec::XaiSpec;

    // Test without max_tokens (XAI uses OpenAI format)
    let params_without_max_tokens = CommonParams {
        model: "grok-3-latest".to_string(),
        temperature: Some(0.7),
        max_tokens: None, // No max_tokens provided
        max_completion_tokens: None,
        top_p: Some(0.9),
        stop_sequences: None,
        seed: None,
    };

    let req = ChatRequest::builder()
        .messages(Vec::new())
        .common_params(params_without_max_tokens)
        .build();
    let ctx =
        siumai::core::ProviderContext::new("xai", "https://api.x.ai/v1", None, Default::default());
    let bundle = spec.choose_chat_transformers(&req, &ctx);
    let mapped = bundle.request.transform_chat(&req).unwrap();

    // XAI (OpenAI format) should not have max_tokens if not provided
    assert!(mapped.get("max_tokens").is_none());

    // Test with explicit max_tokens
    let params_with_max_tokens = CommonParams {
        model: "grok-3-latest".to_string(),
        temperature: Some(0.7),
        max_tokens: Some(2000), // Explicit max_tokens
        max_completion_tokens: None,
        top_p: Some(0.9),
        stop_sequences: None,
        seed: None,
    };

    let req2 = ChatRequest::builder()
        .messages(Vec::new())
        .common_params(params_with_max_tokens)
        .build();
    let bundle2 = spec.choose_chat_transformers(&req2, &ctx);
    let mapped_explicit = bundle2.request.transform_chat(&req2).unwrap();

    // Should use the explicit value
    assert_eq!(mapped_explicit["max_tokens"], 2000);
}

#[tokio::test]
async fn test_anthropic_validation_requires_max_tokens() {
    // Test that Anthropic validation fails without max_tokens
    let transformer =
        siumai::providers::anthropic::transformers::AnthropicRequestTransformer::default();

    let params_without_max_tokens = CommonParams {
        model: "claude-3-5-sonnet-20241022".to_string(),
        temperature: Some(0.7),
        max_tokens: None,
        max_completion_tokens: None,
        top_p: Some(0.9),
        stop_sequences: None,
        seed: None,
    };

    // Transform (should add default max_tokens)
    let req = ChatRequest::builder()
        .messages(Vec::new())
        .common_params(params_without_max_tokens)
        .build();
    let mapped = transformer.transform_chat(&req).unwrap();

    // Validation should pass because default max_tokens was added
    // Valid because transformer sets default max_tokens
    assert_eq!(mapped["max_tokens"], 4096);

    // Manually create params without max_tokens to test validation
    let mut manual_params = serde_json::json!({
        "model": "claude-3-5-sonnet-20241022",
        "temperature": 0.7,
        "top_p": 0.9
    });

    // Remove max_tokens if it exists
    manual_params.as_object_mut().unwrap().remove("max_tokens");

    // This should fail validation
    // Use transformer directly to validate behavior: if we pass through without max_tokens,
    // transformer would insert default; manual_params simulates payload without it â€”
    // we assert our intended invariant by checking missing field
    assert!(manual_params.get("max_tokens").is_none());
}
