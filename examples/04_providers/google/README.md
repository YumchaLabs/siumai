# üß† Google Gemini Provider

This directory contains examples demonstrating Google Gemini's capabilities through the Siumai library. Gemini offers powerful multimodal capabilities and competitive performance.

## üìã Available Examples

### üöÄ [Basic Usage](./basic_usage.rs)
**Introduction to Google Gemini capabilities**

Features demonstrated:
- Gemini model selection and configuration
- Basic chat functionality with Gemini
- Parameter optimization for different use cases
- Understanding Gemini's strengths and limitations

```bash
# Run the basic usage example
cargo run --example google_basic_usage
```

**Key Learning Points:**
- How to configure Gemini models
- Understanding Gemini's capabilities
- Cost and performance characteristics
- Best practices for Gemini usage

## üöÄ Getting Started

### Prerequisites

1. **Get your Google AI API key:**
   - Visit [Google AI Studio](https://aistudio.google.com/)
   - Create an API key
   - Set the environment variable:
   ```bash
   export GOOGLE_API_KEY="your-google-api-key"
   ```

2. **Install dependencies:**
   ```bash
   cargo build --examples
   ```

### Quick Start

```rust
use siumai::prelude::*;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let ai = LlmBuilder::new()
        .google()
        .api_key(&std::env::var("GOOGLE_API_KEY")?)
        .model("gemini-1.5-flash")
        .temperature(0.7)
        .build()
        .await?;

    let messages = vec![
        user!("Explain quantum computing in simple terms")
    ];

    let response = ai.chat(messages).await?;
    println!("Gemini: {}", response.content_text().unwrap_or_default());
    
    Ok(())
}
```

## üéØ Gemini Model Guide

### Available Models

| Model | Best For | Speed | Cost |
|-------|----------|-------|------|
| **gemini-1.5-flash** | Fast responses, general tasks | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| **gemini-1.5-pro** | Complex reasoning, high quality | ‚≠ê‚≠ê | ‚≠ê‚≠ê |

### When to Use Gemini

**Gemini 1.5 Flash:**
- Quick responses and general tasks
- Cost-effective applications
- High-volume processing
- Real-time applications

**Gemini 1.5 Pro:**
- Complex reasoning tasks
- High-quality content generation
- Advanced analysis and research
- Multimodal processing

## üåü Gemini's Unique Strengths

### Multimodal Capabilities
- Native support for text, images, and documents
- Excellent vision understanding
- Document analysis and processing
- Code understanding across images

### Performance
- Fast response times
- Competitive pricing
- Good reasoning capabilities
- Large context windows

### Integration
- Google ecosystem integration
- Easy setup and configuration
- Reliable API performance
- Good documentation

## üí∞ Cost Optimization

### Model Selection Strategy
```rust
// Use Flash for most tasks
let fast_ai = LlmBuilder::new()
    .google()
    .model("gemini-1.5-flash")
    .max_tokens(500)
    .build().await?;

// Use Pro for complex tasks
let advanced_ai = LlmBuilder::new()
    .google()
    .model("gemini-1.5-pro")
    .max_tokens(1000)
    .build().await?;
```

### Best Practices
- Choose Flash for general tasks
- Use Pro only for complex reasoning
- Monitor token usage
- Implement response caching
- Set appropriate token limits

## üîó Related Resources

- [Google AI Documentation](https://ai.google.dev/)
- [Gemini API Reference](https://ai.google.dev/api)
- [Google AI Studio](https://aistudio.google.com/)
- [Pricing Information](https://ai.google.dev/pricing)

## üéØ Next Steps

1. **Start with Basic Usage**
   - Run the basic usage example
   - Try different models and parameters
   - Experiment with various prompts

2. **Explore Multimodal Features**
   - Try image analysis capabilities
   - Test document processing
   - Experiment with vision tasks

3. **Production Integration**
   - Implement proper error handling
   - Add usage monitoring
   - Set up cost tracking
   - Create fallback mechanisms

---

**Ready to explore Gemini? Start with [basic_usage.rs](basic_usage.rs)! üöÄ**
