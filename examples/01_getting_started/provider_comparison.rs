//! üîç Provider Comparison - Understanding Different AI Providers
//!
//! This example demonstrates the differences between AI providers:
//! - Performance characteristics
//! - Model capabilities
//! - Cost considerations
//! - Use case recommendations
//!
//! Before running, set your API keys:
//! ```bash
//! export OPENAI_API_KEY="your-key"
//! export ANTHROPIC_API_KEY="your-key"
//! ```
//!
//! Run with:
//! ```bash
//! cargo run --example provider_comparison
//! ```

use siumai::models;
use siumai::prelude::*;
use std::time::Instant;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    println!("üîç Provider Comparison - Understanding Different AI Providers\n");

    // Test the same prompt with different providers
    let test_prompt = "Explain the concept of recursion in programming with a simple example.";

    compare_providers(test_prompt).await;
    demonstrate_provider_strengths().await;
    show_cost_considerations().await;
    provide_recommendations().await;

    println!("\n‚úÖ Provider comparison completed!");
    Ok(())
}

/// Compare the same prompt across different providers
async fn compare_providers(prompt: &str) {
    println!("‚öñÔ∏è  Provider Performance Comparison:\n");
    println!("   Test prompt: \"{prompt}\"\n");

    // Test OpenAI
    test_provider_performance("OpenAI", || async {
        if let Ok(api_key) = std::env::var("OPENAI_API_KEY") {
            let client = LlmBuilder::new()
                .openai()
                .api_key(&api_key)
                .model(models::openai::GPT_4O_MINI)
                .temperature(0.7)
                .build()
                .await?;

            let messages = vec![user!(prompt)];
            let response = client.chat(messages).await?;
            Ok(response)
        } else {
            Err(LlmError::AuthenticationError("No API key".to_string()))
        }
    })
    .await;

    // Test Anthropic
    test_provider_performance("Anthropic", || async {
        if let Ok(api_key) = std::env::var("ANTHROPIC_API_KEY") {
            let client = LlmBuilder::new()
                .anthropic()
                .api_key(&api_key)
                .model(models::anthropic::CLAUDE_HAIKU_3_5)
                .temperature(0.7)
                .build()
                .await?;

            let messages = vec![user!(prompt)];
            let response = client.chat(messages).await?;
            Ok(response)
        } else {
            Err(LlmError::AuthenticationError("No API key".to_string()))
        }
    })
    .await;

    // Test Ollama (local)
    test_provider_performance("Ollama", || async {
        let client = LlmBuilder::new()
            .ollama()
            .base_url("http://localhost:11434")
            .model("llama3.2")
            .temperature(0.7)
            .build()
            .await?;

        let messages = vec![user!(prompt)];
        let response = client.chat(messages).await?;
        Ok(response)
    })
    .await;
}

/// Test a single provider's performance
async fn test_provider_performance<F, Fut>(provider_name: &str, test_fn: F)
where
    F: FnOnce() -> Fut,
    Fut: std::future::Future<Output = Result<ChatResponse, LlmError>>,
{
    println!("   üß™ Testing {provider_name}:");

    let start_time = Instant::now();

    match test_fn().await {
        Ok(response) => {
            let duration = start_time.elapsed();

            if let Some(text) = response.content_text() {
                println!("      ‚úÖ Success");
                println!("      ‚è±Ô∏è  Response time: {}ms", duration.as_millis());
                println!("      üìù Response length: {} characters", text.len());

                if let Some(usage) = &response.usage {
                    println!(
                        "      üî¢ Tokens: {} total ({} prompt + {} completion)",
                        usage.total_tokens, usage.prompt_tokens, usage.completion_tokens
                    );
                }

                // Show first 100 characters of response
                let preview = if text.len() > 100 {
                    format!("{}...", &text[..100])
                } else {
                    text.to_string()
                };
                println!("      üí¨ Preview: {preview}");
            }
        }
        Err(e) => {
            println!("      ‚ùå Failed: {e}");
            match provider_name {
                "OpenAI" => println!("      üí° Set OPENAI_API_KEY environment variable"),
                "Anthropic" => println!("      üí° Set ANTHROPIC_API_KEY environment variable"),
                "Ollama" => println!("      üí° Ensure Ollama is running: ollama serve"),
                _ => {}
            }
        }
    }

    println!();
}

/// Demonstrate each provider's unique strengths
async fn demonstrate_provider_strengths() {
    println!("üí™ Provider Strengths:\n");

    println!("   ü§ñ OpenAI:");
    println!("      ‚Ä¢ Most popular and well-documented");
    println!("      ‚Ä¢ Excellent general-purpose performance");
    println!("      ‚Ä¢ Strong multimodal capabilities (vision, audio)");
    println!("      ‚Ä¢ Large ecosystem and community");
    println!("      ‚Ä¢ Best for: General applications, prototyping");

    println!("\n   üß† Anthropic (Claude):");
    println!("      ‚Ä¢ Excellent reasoning and analysis");
    println!("      ‚Ä¢ Strong safety and alignment focus");
    println!("      ‚Ä¢ Great for complex, nuanced tasks");
    println!("      ‚Ä¢ Transparent thinking process");
    println!("      ‚Ä¢ Best for: Research, analysis, complex reasoning");

    println!("\n   üè† Ollama (Local):");
    println!("      ‚Ä¢ Complete privacy and data control");
    println!("      ‚Ä¢ No API costs after setup");
    println!("      ‚Ä¢ Works offline");
    println!("      ‚Ä¢ Customizable and fine-tunable");
    println!("      ‚Ä¢ Best for: Privacy-sensitive applications, development");

    println!("\n   ‚ö° Groq (if available):");
    println!("      ‚Ä¢ Extremely fast inference");
    println!("      ‚Ä¢ Cost-effective for high-volume usage");
    println!("      ‚Ä¢ Great for real-time applications");
    println!("      ‚Ä¢ Best for: High-throughput, latency-sensitive apps");
}

/// Show cost considerations for different providers
async fn show_cost_considerations() {
    println!("\nüí∞ Cost Considerations:\n");

    println!("   üìä Approximate Pricing (per 1M tokens):");
    println!("      ‚Ä¢ OpenAI GPT-4o-mini: ~$0.15 input, ~$0.60 output");
    println!("      ‚Ä¢ Anthropic Claude Haiku: ~$0.25 input, ~$1.25 output");
    println!("      ‚Ä¢ Ollama: Free after hardware investment");
    println!("      ‚Ä¢ Groq: ~$0.05 input, ~$0.08 output (very fast)");

    println!("\n   üí° Cost Optimization Tips:");
    println!("      ‚Ä¢ Use smaller models for simple tasks");
    println!("      ‚Ä¢ Implement caching for repeated queries");
    println!("      ‚Ä¢ Monitor token usage in production");
    println!("      ‚Ä¢ Consider local models for development");
    println!("      ‚Ä¢ Use streaming to provide better UX while processing");
}

/// Provide recommendations for different use cases
async fn provide_recommendations() {
    println!("\nüéØ Use Case Recommendations:\n");

    println!("   üöÄ Getting Started / Prototyping:");
    println!("      ‚Üí OpenAI GPT-4o-mini");
    println!("      ‚Ä¢ Easy to use, well-documented");
    println!("      ‚Ä¢ Good balance of cost and performance");

    println!("\n   üè¢ Production Applications:");
    println!("      ‚Üí Multiple providers with fallback");
    println!("      ‚Ä¢ Primary: Based on your specific needs");
    println!("      ‚Ä¢ Fallback: Different provider for reliability");

    println!("\n   üîí Privacy-Sensitive Applications:");
    println!("      ‚Üí Ollama (local deployment)");
    println!("      ‚Ä¢ Complete data control");
    println!("      ‚Ä¢ No external API calls");

    println!("\n   üìä High-Volume / Real-Time:");
    println!("      ‚Üí Groq or OpenAI with caching");
    println!("      ‚Ä¢ Fast response times");
    println!("      ‚Ä¢ Cost-effective at scale");

    println!("\n   üß™ Research / Complex Analysis:");
    println!("      ‚Üí Anthropic Claude");
    println!("      ‚Ä¢ Superior reasoning capabilities");
    println!("      ‚Ä¢ Transparent thinking process");

    println!("\n   üíª Development / Testing:");
    println!("      ‚Üí Ollama for development, cloud for production");
    println!("      ‚Ä¢ Free local testing");
    println!("      ‚Ä¢ Easy transition to production");
}

/// üéØ Key Comparison Points:
///
/// Performance Factors:
/// - Response time and latency
/// - Quality and accuracy
/// - Token efficiency
/// - Reliability and uptime
///
/// Cost Factors:
/// - Per-token pricing
/// - Volume discounts
/// - Hidden costs (rate limits, etc.)
/// - Total cost of ownership
///
/// Feature Differences:
/// - Model capabilities
/// - Multimodal support
/// - Context window size
/// - Special features (thinking, tools, etc.)
///
/// Selection Criteria:
/// 1. Define your primary use case
/// 2. Consider performance requirements
/// 3. Evaluate cost constraints
/// 4. Test with your specific data
/// 5. Plan for scaling and reliability
///
/// Next Steps:
/// - `basic_usage.rs`: Learn core functionality
/// - ../`02_core_features/`: Explore advanced features
/// - ../`04_providers/`: Provider-specific capabilities
const fn _documentation() {}
